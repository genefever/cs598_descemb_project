{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12eeb976-aec8-4203-979f-ed85033f9aa9",
   "metadata": {},
   "source": [
    "# CS598 Project: Enhancing Healthcare Predictive Models through Text-Based EHR Code Embedding\n",
    "\n",
    "**Name:** Gene Horecka  \n",
    "**Email:** [geneeh2@illinois.edu](mailto:geneeh2@illinois.edu)  \n",
    "**Course:** CS 598 Deep Learning for Healthcare - Spring 2024\n",
    "## Project Github Link: [https://github.com/genefever/cs598_descemb_project](https://github.com/genefever/cs598_descemb_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719c0ca",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "The paper \"[Unifying Heterogeneous Electronic Health Records Systems via Text-Based Code Embedding](https://arxiv.org/abs/2108.03625)\" addresses the significant challenge of heterogeneity in Electronic Health Records (EHR) systems. These systems, essential for modern healthcare, often differ in their coding and formatting of medical data, which hampers the development and application of predictive models across different institutions or datasets.\n",
    "\n",
    "The primary contribution of this paper is the development of a novel framework named [Description-based Embedding (DescEmb)](https://github.com/hoon9405/DescEmb). This framework uses neural language models to create a unified, code-agnostic text-based representation of medical data. By transforming various coding formats into a consistent text-based embedding, DescEmb allows for more flexible and effective application of deep learning models across diverse EHR systems. This approach notably enhances the performance of predictive healthcare models, demonstrating superior results in several experimental setups compared to traditional code-based embedding methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64856b9",
   "metadata": {},
   "source": [
    "\n",
    "# Scope of Reproducibility\n",
    "\n",
    "The scope of reproducibility for the paper \"[Unifying Heterogeneous Electronic Health Records Systems via Text-Based Code Embedding](https://arxiv.org/abs/2108.03625)\" entails verifying the claims of improved predictive performance through the implementation of the DescEmb framework. This framework leverages a neural language model to convert medical codes into a unified, text-based embedding, which is purported to enhance predictive healthcare research without the constraints imposed by diverse EHR systems.\n",
    "\n",
    "## Key Claims for Reproduction:\n",
    "1. **Unified Learning Across Diverse EHR Formats:** DescEmb can unify learning across various EHR systems without needing individualized pre-processing or domain-specific knowledge, due to its text-based nature.\n",
    "2. **Superior Predictive Performance:** The framework demonstrates better or comparable predictive performance than traditional code-based approaches across several clinical prediction tasks.\n",
    "3. **Efficient Deployment in Diverse Environments:** The text-based approach allows models trained with DescEmb to be easily transferred and applied across different hospitals with differing EHR systems.\n",
    "\n",
    "The reproducibility effort will focus on these claims by attempting to replicate the experiments outlined in the original paper using the datasets and code provided in the project's [GitHub repository](https://github.com/hoon9405/DescEmb). The process will involve re-running the model training and evaluation procedures to verify the reported performance improvements and the operational flexibility of the DescEmb approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ee017-b71e-4406-ba6a-ff4abe8fc620",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "## Requirements\n",
    "\n",
    "- PyTorch version >= 1.8.1\n",
    "- Python version >= 3.7\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "To replicate the preprocessing and modeling described in the project, the following environment must be set up:\n",
    "\n",
    "1. **Conda Environment**: Use the provided `environment.yml` file to create a Conda environment. This will install all required dependencies, including Python and PyTorch. Run the following command in your terminal:\n",
    "\n",
    "   ```bash\n",
    "   conda env create -f environment.yml\n",
    "\n",
    "\n",
    "\n",
    "2. **Activate the Environment**\n",
    "\n",
    "   ```bash\n",
    "   conda activate descemb\n",
    "\n",
    "## Data\n",
    "\n",
    "### Data Description\n",
    "\n",
    "The datasets used in this project are MIMIC-III and eICU, which are publicly available on the PhysioNet repository. These datasets include comprehensive data from intensive care units (ICUs), such as time-stamped records of medical events, lab results, medications, and more, recorded in different medical code systems.\n",
    "\n",
    "- [**MIMIC-III**](https://physionet.org/content/iii/1.4/): Contains data for over 60,000 ICU stays at Beth Israel Deaconess Medical Center between 2001 and 2012. It includes information such as lab measurements, medication orders, and diagnostic codes.\n",
    "- [**eICU**](https://physionet.org/content/eicu-crd/2.0/): A multi-center dataset containing data for over 200,000 ICU stays across the United States between 2014 and 2015. It includes similar types of data to MIMIC-III but is structured differently.\n",
    "- [**ccs_multi_dx_tool_2015**](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/Multi_Level_CCS_2015.zip): The Clinical Classifications Software (CCS) 2015 dataset groups ICD-9-CM diagnosis and procedure codes into clinically meaningful categories that are useful for health data analysis and research.\n",
    "- [**icd10cmtoicd9gem**](https://data.nber.org/gem/icd10cmtoicd9gem.csv): The `icd10cmtoicd9gem.csv` file is a mapping table that converts ICD-10-CM codes to ICD-9-CM codes.\n",
    "\n",
    "### Data Access\n",
    "\n",
    "The datasets utilized in this project, MIMIC-III and eICU, are publicly available via PhysioNet. Users must adhere to licensing agreements and data usage policies, including the requirement for completing a training course on data handling. Detailed instructions for data access are as follows:\n",
    "\n",
    "- **MIMIC-III** and **eICU**: Access these datasets by registering and completing the required data usage agreement at [PhysioNet](https://physionet.org/). After gaining access, download the data directly from their respective project pages.\n",
    "\n",
    "- Alternatively, you can use the public [**MIMIC-III Demo Dataset**](https://physionet.org/content/mimiciii-demo/1.4/) and [**eICU Demo Dataset**](https://www.physionet.org/content/eicu-crd-demo/2.0.1/) without having to create an account, albeit it will just be a fraction of the real dataset. However, this is a good option if you would like to get started quickly and run the computations on a CPU.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The preparation of the datasets for training involves several steps, from downloading the data to preprocessing it into a usable format. Here’s how you can prepare your data:\n",
    "\n",
    "1. **Download the Data**: After obtaining the necessary permissions, download the datasets from PhysioNet.\n",
    "2. **Organize the Data**: Arrange the downloaded files according to the directory structure below:\n",
    "```\n",
    "data_input_path\n",
    "├─ mimic\n",
    "│  ├─ ADMISSIONS.csv\n",
    "│  ├─ PATIENTS.csv\n",
    "│  ├─ ICUSYAYS.csv\n",
    "│  ├─ LABEVENTES.csv\n",
    "│  ├─ PRESCRIPTIONS.csv\n",
    "│  ├─ PROCEDURES.csv\n",
    "│  ├─ INPUTEVENTS_CV.csv\n",
    "│  ├─ INPUTEVENTS_MV.csv\n",
    "│  ├─ D_ITEMDS.csv\n",
    "│  ├─ D_ICD_PROCEDURES.csv\n",
    "│  └─ D_LABITEMBS.csv\n",
    "├─ eicu\n",
    "│  ├─ diagnosis.csv\n",
    "│  ├─ infusionDrug.csv\n",
    "│  ├─ lab.csv\n",
    "│  ├─ medication.csv\n",
    "│  └─ patient.csv\n",
    "├─ ccs_multi_dx_tool_2015.csv\n",
    "└─ icd10cmtoicd9gem.csv\n",
    "\n",
    "```\n",
    "```\n",
    "data_output_path\n",
    "├─mimic\n",
    "├─eicu\n",
    "├─pooled\n",
    "├─label\n",
    "└─fold\n",
    "```\n",
    "\n",
    "3. **Preprocess the Data**: Use the following Python script to execute the preprocessing steps. This script automates the process of converting raw datasets into a format ready for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29c46dd8-9da6-4e25-aace-9ef771cf6a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working directory .. :  /Users/genehorecka/Documents/01 UIUC/CS598/Project/cs598_descemb_project\n",
      "create dest path.. datasets/data_output_path/mlm\n",
      "Destination directory is set to datasets/data_output_path/mlm\n",
      "Data directory is set to datasets/data_input_path/mimic\n",
      "length of PATIENTS.csv  :  100\n",
      "length of ICUSTAYS.csv  :  136\n",
      "length of DIAGNOSIS_ICD.csv  :  136\n",
      "length of icus  : 72\n",
      "readmission value counts : readmission\n",
      "0              64\n",
      "1               4\n",
      "Name: count, dtype: int64\n",
      "dx_label_mapping.pkl dx mapping pkl save___\n",
      "average length:  7.432835820895522\n",
      "dx freqeuncy [ 0  1  0  1  2 14  4 12 10 14  3  3  0  3]\n",
      "max length:  13\n",
      "min length:  1\n",
      "data preparation initialization .. mimiciii lab\n",
      "df_load ! .. mimiciii lab\n",
      "data preparation initialization .. mimiciii med\n",
      "df_load ! .. mimiciii med\n",
      "data preparation initialization .. mimiciii inf\n",
      "mimic INPUTEVENTS merge!\n",
      "df_load ! .. mimiciii inf\n",
      "data preparation finish for three tables \n",
      " second preparation start soon..\n",
      "lab med inf three categories merged in one!\n",
      "sortbyoffset\n",
      "Preprocessing completed.\n",
      "Writing mimiciii_df.pkl to datasets/data_output_path/mlm\n",
      "label numpy file save to  datasets/data_output_path/mlm/mimiciii/label/mortality.npy\n",
      "label numpy file save to  datasets/data_output_path/mlm/mimiciii/label/readmission.npy\n",
      "label numpy file save to  datasets/data_output_path/mlm/mimiciii/label/los_3day.npy\n",
      "label numpy file save to  datasets/data_output_path/mlm/mimiciii/label/los_7day.npy\n",
      "['1' '10' '12' '13' '16' '17' '18' '2' '3' '4' '5' '6' '7' '8' '9']\n",
      "label numpy file save to  datasets/data_output_path/mlm/mimiciii/label/diagnosis.npy\n",
      "seed :  1\n",
      "mortality train and test split\n",
      "X fold_task value counts \n",
      " mortality_fold\n",
      "1    66\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2 label distribution:\n",
      "mortality\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "0 label distribution:\n",
      "mortality\n",
      "0    0.666667\n",
      "1    0.333333\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "1 label distribution:\n",
      "mortality\n",
      "0    0.787234\n",
      "1    0.212766\n",
      "Name: proportion, dtype: float64\n",
      "fold split mimiciii with mortality done \n",
      " mortality_fold\n",
      "1    47\n",
      "2    13\n",
      "0     6\n",
      "Name: count, dtype: int64\n",
      "readmission train and test split\n",
      "X fold_task value counts \n",
      " readmission_fold\n",
      "1    66\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2 label distribution:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "1 label distribution:\n",
      "readmission\n",
      "0    0.9375\n",
      "1    0.0625\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "0 label distribution:\n",
      "readmission\n",
      "0    0.8\n",
      "1    0.2\n",
      "Name: proportion, dtype: float64\n",
      "fold split mimiciii with readmission done \n",
      " readmission_fold\n",
      "1    48\n",
      "2    13\n",
      "0     5\n",
      "Name: count, dtype: int64\n",
      "los_3day train and test split\n",
      "X fold_task value counts \n",
      " los_3day_fold\n",
      "1    66\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2 label distribution:\n",
      "los_3day\n",
      "0    0.846154\n",
      "1    0.153846\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "1 label distribution:\n",
      "los_3day\n",
      "0    0.688889\n",
      "1    0.311111\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "0 label distribution:\n",
      "los_3day\n",
      "0    0.625\n",
      "1    0.375\n",
      "Name: proportion, dtype: float64\n",
      "fold split mimiciii with los_3day done \n",
      " los_3day_fold\n",
      "1    45\n",
      "2    13\n",
      "0     8\n",
      "Name: count, dtype: int64\n",
      "los_7day train and test split\n",
      "X fold_task value counts \n",
      " los_7day_fold\n",
      "1    66\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2 label distribution:\n",
      "los_7day\n",
      "0    0.846154\n",
      "1    0.153846\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "0 label distribution:\n",
      "los_7day\n",
      "0    0.875\n",
      "1    0.125\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "1 label distribution:\n",
      "los_7day\n",
      "0    0.844444\n",
      "1    0.155556\n",
      "Name: proportion, dtype: float64\n",
      "fold split mimiciii with los_7day done \n",
      " los_7day_fold\n",
      "1    45\n",
      "2    13\n",
      "0     8\n",
      "Name: count, dtype: int64\n",
      "diagnosis multi label stratified split\n",
      "Diagnosis multi-label stratified split results:\n",
      "diagnosis_fold\n",
      "0    12\n",
      "1    45\n",
      "2     9\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fold 0 label distribution:\n",
      "diagnosis\n",
      "[9, 18, 12, 8, 4, 17, 3, 6, 1, 10, 5, 7, 16]    0.083333\n",
      "[9, 1, 8, 4, 7, 10, 17]                         0.083333\n",
      "[18, 8, 4, 3, 7, 5, 17, 16]                     0.083333\n",
      "[2]                                             0.083333\n",
      "[18, 3, 6, 7, 10, 5, 17, 16]                    0.083333\n",
      "[9, 1, 12, 8, 4, 3, 6, 7, 10]                   0.083333\n",
      "[9, 12, 8, 7, 4, 3, 6, 1, 10, 5, 17]            0.083333\n",
      "[18, 2, 8, 4, 3, 7, 5, 16]                      0.083333\n",
      "[1, 13, 8, 3, 6, 7, 10]                         0.083333\n",
      "[9, 13, 8, 3, 1, 10, 5, 17]                     0.083333\n",
      "[9, 18, 2, 8, 4, 3, 1, 10, 7]                   0.083333\n",
      "[9, 1, 18, 2, 8, 4, 3, 10]                      0.083333\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 1 label distribution:\n",
      "diagnosis\n",
      "[1, 3, 7, 10, 17]                               0.022222\n",
      "[4, 7, 17]                                      0.022222\n",
      "[9, 1, 18, 13, 10, 8, 4, 3, 6, 7, 5, 17, 16]    0.022222\n",
      "[4, 7, 9, 8]                                    0.022222\n",
      "[9, 8, 17, 4, 3, 1, 10, 5, 7]                   0.022222\n",
      "[9, 8, 4, 3, 7, 10, 17]                         0.022222\n",
      "[9, 1, 18, 12, 8, 3, 7, 10, 16]                 0.022222\n",
      "[9, 18, 2, 13, 8, 3, 7, 10]                     0.022222\n",
      "[1, 3, 6, 7, 10, 17]                            0.022222\n",
      "[1, 13, 8, 6, 7]                                0.022222\n",
      "[18, 8, 3, 7, 10]                               0.022222\n",
      "[12, 8, 17, 4, 3, 6, 1, 10, 7, 16]              0.022222\n",
      "[18, 2, 8, 4, 3, 7, 10, 17, 16]                 0.022222\n",
      "[4, 5, 17, 16]                                  0.022222\n",
      "[1, 13, 8, 6, 3, 10, 7]                         0.022222\n",
      "[9, 1, 2, 8, 3, 6, 7, 10, 17]                   0.022222\n",
      "[18, 12, 8, 17, 4, 3, 6, 1, 10, 7, 16]          0.022222\n",
      "[18, 13, 4, 3, 7, 10, 16]                       0.022222\n",
      "[9, 2, 4, 3, 1]                                 0.022222\n",
      "[9, 8, 3, 6, 7, 5]                              0.022222\n",
      "[1, 2, 8, 7, 16]                                0.022222\n",
      "[2, 7, 8, 3, 1, 17, 16]                         0.022222\n",
      "[9, 1, 8, 4, 7]                                 0.022222\n",
      "[9, 18, 3, 10, 5]                               0.022222\n",
      "[9, 18, 1, 2, 12, 8, 3, 7, 17]                  0.022222\n",
      "[9, 4, 3, 1, 5]                                 0.022222\n",
      "[9, 1, 18, 8, 3, 6, 7, 10, 5, 17]               0.022222\n",
      "[18, 12, 8, 4, 3, 7, 5, 17]                     0.022222\n",
      "[9, 1, 8, 3, 7]                                 0.022222\n",
      "[2, 4, 12, 10, 7]                               0.022222\n",
      "[9, 12, 4, 3, 1, 10, 7]                         0.022222\n",
      "[2, 8, 4, 3, 10, 7]                             0.022222\n",
      "[18, 10, 8, 3, 7, 5, 16]                        0.022222\n",
      "[9, 18, 2, 8, 7, 10, 5]                         0.022222\n",
      "[18, 2, 8, 4, 3, 7]                             0.022222\n",
      "[8, 3, 6, 7, 5]                                 0.022222\n",
      "[9, 18, 8, 4, 6, 3, 5, 7, 16]                   0.022222\n",
      "[2, 8, 4, 3, 7, 10, 16]                         0.022222\n",
      "[9, 18, 4, 17, 3, 1, 5, 7, 16]                  0.022222\n",
      "[9, 4, 1, 10, 7]                                0.022222\n",
      "[9, 1, 2, 4, 6, 7, 10, 5, 17]                   0.022222\n",
      "[9, 12, 8, 17, 3, 6, 1, 10, 5, 7]               0.022222\n",
      "[12, 7, 8, 3, 6, 1, 10, 5, 17]                  0.022222\n",
      "[9, 18, 12, 8, 4, 17, 3, 6, 1, 10, 5, 7, 16]    0.022222\n",
      "[9, 1, 8, 4, 6, 3, 10, 7]                       0.022222\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Fold 2 label distribution:\n",
      "diagnosis\n",
      "[18, 8, 4, 3, 6, 7, 5, 16]              0.111111\n",
      "[9, 18, 8, 3, 6, 7, 10, 5, 16]          0.111111\n",
      "[18, 2, 8, 4, 3, 7, 10, 5, 16]          0.111111\n",
      "[1, 3, 7, 10, 17]                       0.111111\n",
      "[9, 12, 7, 8, 4, 3, 6, 1, 10, 5, 17]    0.111111\n",
      "[9, 18, 2, 12, 8, 4, 3, 7]              0.111111\n",
      "[2, 12, 8, 3, 7, 10, 5]                 0.111111\n",
      "[9, 8, 4, 6, 3, 10, 7]                  0.111111\n",
      "[9, 13, 8, 7, 4, 1, 10, 5, 17]          0.111111\n",
      "Name: proportion, dtype: float64\n",
      "preparing split for few-shot learning\n",
      "Ratio: 10\n",
      "mortality 10\n",
      "mortality: Excluded 38 samples from train, 11 samples from valid\n",
      "readmission 10\n",
      "readmission: Excluded 39 samples from train, 11 samples from valid\n",
      "los_3day 10\n",
      "los_3day: Excluded 36 samples from train, 11 samples from valid\n",
      "los_7day 10\n",
      "los_7day: Excluded 36 samples from train, 11 samples from valid\n",
      "diagnosis 10\n",
      "diagnosis: Excluded 36 samples from train, 8 samples from valid\n",
      "\n",
      "mortality label distribution:\n",
      "Train set:\n",
      "mortality\n",
      "0    0.777778\n",
      "1    0.222222\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "mortality\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "readmission label distribution:\n",
      "Train set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_3day label distribution:\n",
      "Train set:\n",
      "los_3day\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_3day\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_7day label distribution:\n",
      "Train set:\n",
      "los_7day\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_7day\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "diagnosis label distribution:\n",
      "Train set:\n",
      "diagnosis\n",
      "[1, 3, 7, 10, 17]                 0.111111\n",
      "[9, 1, 8, 3, 7]                   0.111111\n",
      "[2, 8, 4, 3, 10, 7]               0.111111\n",
      "[18, 10, 8, 3, 7, 5, 16]          0.111111\n",
      "[18, 2, 8, 4, 3, 7]               0.111111\n",
      "[8, 3, 6, 7, 5]                   0.111111\n",
      "[9, 18, 4, 17, 3, 1, 5, 7, 16]    0.111111\n",
      "[1, 13, 8, 6, 3, 10, 7]           0.111111\n",
      "[1, 2, 8, 7, 16]                  0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "diagnosis\n",
      "[9, 18, 2, 12, 8, 4, 3, 7]    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Ratio: 30\n",
      "mortality 30\n",
      "mortality: Excluded 38 samples from train, 11 samples from valid\n",
      "readmission 30\n",
      "readmission: Excluded 39 samples from train, 11 samples from valid\n",
      "los_3day 30\n",
      "los_3day: Excluded 36 samples from train, 11 samples from valid\n",
      "los_7day 30\n",
      "los_7day: Excluded 36 samples from train, 11 samples from valid\n",
      "diagnosis 30\n",
      "diagnosis: Excluded 36 samples from train, 8 samples from valid\n",
      "\n",
      "mortality label distribution:\n",
      "Train set:\n",
      "mortality\n",
      "0    0.777778\n",
      "1    0.222222\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "mortality\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "readmission label distribution:\n",
      "Train set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_3day label distribution:\n",
      "Train set:\n",
      "los_3day\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_3day\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_7day label distribution:\n",
      "Train set:\n",
      "los_7day\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_7day\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "diagnosis label distribution:\n",
      "Train set:\n",
      "diagnosis\n",
      "[1, 3, 7, 10, 17]                 0.111111\n",
      "[9, 1, 8, 3, 7]                   0.111111\n",
      "[2, 8, 4, 3, 10, 7]               0.111111\n",
      "[18, 10, 8, 3, 7, 5, 16]          0.111111\n",
      "[18, 2, 8, 4, 3, 7]               0.111111\n",
      "[8, 3, 6, 7, 5]                   0.111111\n",
      "[9, 18, 4, 17, 3, 1, 5, 7, 16]    0.111111\n",
      "[1, 13, 8, 6, 3, 10, 7]           0.111111\n",
      "[1, 2, 8, 7, 16]                  0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "diagnosis\n",
      "[9, 18, 2, 12, 8, 4, 3, 7]    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Ratio: 50\n",
      "mortality 50\n",
      "mortality: Excluded 38 samples from train, 11 samples from valid\n",
      "readmission 50\n",
      "readmission: Excluded 39 samples from train, 11 samples from valid\n",
      "los_3day 50\n",
      "los_3day: Excluded 36 samples from train, 11 samples from valid\n",
      "los_7day 50\n",
      "los_7day: Excluded 36 samples from train, 11 samples from valid\n",
      "diagnosis 50\n",
      "diagnosis: Excluded 36 samples from train, 8 samples from valid\n",
      "\n",
      "mortality label distribution:\n",
      "Train set:\n",
      "mortality\n",
      "0    0.777778\n",
      "1    0.222222\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "mortality\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "readmission label distribution:\n",
      "Train set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_3day label distribution:\n",
      "Train set:\n",
      "los_3day\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_3day\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_7day label distribution:\n",
      "Train set:\n",
      "los_7day\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_7day\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "diagnosis label distribution:\n",
      "Train set:\n",
      "diagnosis\n",
      "[1, 3, 7, 10, 17]                 0.111111\n",
      "[9, 1, 8, 3, 7]                   0.111111\n",
      "[2, 8, 4, 3, 10, 7]               0.111111\n",
      "[18, 10, 8, 3, 7, 5, 16]          0.111111\n",
      "[18, 2, 8, 4, 3, 7]               0.111111\n",
      "[8, 3, 6, 7, 5]                   0.111111\n",
      "[9, 18, 4, 17, 3, 1, 5, 7, 16]    0.111111\n",
      "[1, 13, 8, 6, 3, 10, 7]           0.111111\n",
      "[1, 2, 8, 7, 16]                  0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "diagnosis\n",
      "[9, 18, 2, 12, 8, 4, 3, 7]    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Ratio: 70\n",
      "mortality 70\n",
      "mortality: Excluded 38 samples from train, 11 samples from valid\n",
      "readmission 70\n",
      "readmission: Excluded 39 samples from train, 11 samples from valid\n",
      "los_3day 70\n",
      "los_3day: Excluded 36 samples from train, 11 samples from valid\n",
      "los_7day 70\n",
      "los_7day: Excluded 36 samples from train, 11 samples from valid\n",
      "diagnosis 70\n",
      "diagnosis: Excluded 36 samples from train, 8 samples from valid\n",
      "\n",
      "mortality label distribution:\n",
      "Train set:\n",
      "mortality\n",
      "0    0.777778\n",
      "1    0.222222\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "mortality\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "readmission label distribution:\n",
      "Train set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_3day label distribution:\n",
      "Train set:\n",
      "los_3day\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_3day\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_7day label distribution:\n",
      "Train set:\n",
      "los_7day\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_7day\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "diagnosis label distribution:\n",
      "Train set:\n",
      "diagnosis\n",
      "[1, 3, 7, 10, 17]                 0.111111\n",
      "[9, 1, 8, 3, 7]                   0.111111\n",
      "[2, 8, 4, 3, 10, 7]               0.111111\n",
      "[18, 10, 8, 3, 7, 5, 16]          0.111111\n",
      "[18, 2, 8, 4, 3, 7]               0.111111\n",
      "[8, 3, 6, 7, 5]                   0.111111\n",
      "[9, 18, 4, 17, 3, 1, 5, 7, 16]    0.111111\n",
      "[1, 13, 8, 6, 3, 10, 7]           0.111111\n",
      "[1, 2, 8, 7, 16]                  0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "diagnosis\n",
      "[9, 18, 2, 12, 8, 4, 3, 7]    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Ratio: 90\n",
      "mortality 90\n",
      "mortality: Excluded 38 samples from train, 11 samples from valid\n",
      "readmission 90\n",
      "readmission: Excluded 39 samples from train, 11 samples from valid\n",
      "los_3day 90\n",
      "los_3day: Excluded 36 samples from train, 11 samples from valid\n",
      "los_7day 90\n",
      "los_7day: Excluded 36 samples from train, 11 samples from valid\n",
      "diagnosis 90\n",
      "diagnosis: Excluded 36 samples from train, 8 samples from valid\n",
      "\n",
      "mortality label distribution:\n",
      "Train set:\n",
      "mortality\n",
      "0    0.777778\n",
      "1    0.222222\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "mortality\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "readmission label distribution:\n",
      "Train set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_3day label distribution:\n",
      "Train set:\n",
      "los_3day\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_3day\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_7day label distribution:\n",
      "Train set:\n",
      "los_7day\n",
      "0    0.888889\n",
      "1    0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_7day\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "diagnosis label distribution:\n",
      "Train set:\n",
      "diagnosis\n",
      "[1, 3, 7, 10, 17]                 0.111111\n",
      "[9, 1, 8, 3, 7]                   0.111111\n",
      "[2, 8, 4, 3, 10, 7]               0.111111\n",
      "[18, 10, 8, 3, 7, 5, 16]          0.111111\n",
      "[18, 2, 8, 4, 3, 7]               0.111111\n",
      "[8, 3, 6, 7, 5]                   0.111111\n",
      "[9, 18, 4, 17, 3, 1, 5, 7, 16]    0.111111\n",
      "[1, 13, 8, 6, 3, 10, 7]           0.111111\n",
      "[1, 2, 8, 7, 16]                  0.111111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "diagnosis\n",
      "[9, 18, 2, 12, 8, 4, 3, 7]    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Ratio: 100\n",
      "mortality 100\n",
      "mortality: Excluded 0 samples from train, 0 samples from valid\n",
      "readmission 100\n",
      "readmission: Excluded 0 samples from train, 0 samples from valid\n",
      "los_3day 100\n",
      "los_3day: Excluded 0 samples from train, 0 samples from valid\n",
      "los_7day 100\n",
      "los_7day: Excluded 0 samples from train, 0 samples from valid\n",
      "diagnosis 100\n",
      "diagnosis: Excluded 0 samples from train, 0 samples from valid\n",
      "\n",
      "mortality label distribution:\n",
      "Train set:\n",
      "mortality\n",
      "0    0.787234\n",
      "1    0.212766\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "mortality\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "readmission label distribution:\n",
      "Train set:\n",
      "readmission\n",
      "0    0.9375\n",
      "1    0.0625\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "readmission\n",
      "0    1.0\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_3day label distribution:\n",
      "Train set:\n",
      "los_3day\n",
      "0    0.688889\n",
      "1    0.311111\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_3day\n",
      "0    0.846154\n",
      "1    0.153846\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "los_7day label distribution:\n",
      "Train set:\n",
      "los_7day\n",
      "0    0.844444\n",
      "1    0.155556\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "los_7day\n",
      "0    0.846154\n",
      "1    0.153846\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "diagnosis label distribution:\n",
      "Train set:\n",
      "diagnosis\n",
      "[1, 3, 7, 10, 17]                               0.022222\n",
      "[4, 7, 17]                                      0.022222\n",
      "[9, 1, 18, 13, 10, 8, 4, 3, 6, 7, 5, 17, 16]    0.022222\n",
      "[4, 7, 9, 8]                                    0.022222\n",
      "[9, 8, 17, 4, 3, 1, 10, 5, 7]                   0.022222\n",
      "[9, 8, 4, 3, 7, 10, 17]                         0.022222\n",
      "[9, 1, 18, 12, 8, 3, 7, 10, 16]                 0.022222\n",
      "[9, 18, 2, 13, 8, 3, 7, 10]                     0.022222\n",
      "[1, 3, 6, 7, 10, 17]                            0.022222\n",
      "[1, 13, 8, 6, 7]                                0.022222\n",
      "[18, 8, 3, 7, 10]                               0.022222\n",
      "[12, 8, 17, 4, 3, 6, 1, 10, 7, 16]              0.022222\n",
      "[18, 2, 8, 4, 3, 7, 10, 17, 16]                 0.022222\n",
      "[4, 5, 17, 16]                                  0.022222\n",
      "[1, 13, 8, 6, 3, 10, 7]                         0.022222\n",
      "[9, 1, 2, 8, 3, 6, 7, 10, 17]                   0.022222\n",
      "[18, 12, 8, 17, 4, 3, 6, 1, 10, 7, 16]          0.022222\n",
      "[18, 13, 4, 3, 7, 10, 16]                       0.022222\n",
      "[9, 2, 4, 3, 1]                                 0.022222\n",
      "[9, 8, 3, 6, 7, 5]                              0.022222\n",
      "[1, 2, 8, 7, 16]                                0.022222\n",
      "[2, 7, 8, 3, 1, 17, 16]                         0.022222\n",
      "[9, 1, 8, 4, 7]                                 0.022222\n",
      "[9, 18, 3, 10, 5]                               0.022222\n",
      "[9, 18, 1, 2, 12, 8, 3, 7, 17]                  0.022222\n",
      "[9, 4, 3, 1, 5]                                 0.022222\n",
      "[9, 1, 18, 8, 3, 6, 7, 10, 5, 17]               0.022222\n",
      "[18, 12, 8, 4, 3, 7, 5, 17]                     0.022222\n",
      "[9, 1, 8, 3, 7]                                 0.022222\n",
      "[2, 4, 12, 10, 7]                               0.022222\n",
      "[9, 12, 4, 3, 1, 10, 7]                         0.022222\n",
      "[2, 8, 4, 3, 10, 7]                             0.022222\n",
      "[18, 10, 8, 3, 7, 5, 16]                        0.022222\n",
      "[9, 18, 2, 8, 7, 10, 5]                         0.022222\n",
      "[18, 2, 8, 4, 3, 7]                             0.022222\n",
      "[8, 3, 6, 7, 5]                                 0.022222\n",
      "[9, 18, 8, 4, 6, 3, 5, 7, 16]                   0.022222\n",
      "[2, 8, 4, 3, 7, 10, 16]                         0.022222\n",
      "[9, 18, 4, 17, 3, 1, 5, 7, 16]                  0.022222\n",
      "[9, 4, 1, 10, 7]                                0.022222\n",
      "[9, 1, 2, 4, 6, 7, 10, 5, 17]                   0.022222\n",
      "[9, 12, 8, 17, 3, 6, 1, 10, 5, 7]               0.022222\n",
      "[12, 7, 8, 3, 6, 1, 10, 5, 17]                  0.022222\n",
      "[9, 18, 12, 8, 4, 17, 3, 6, 1, 10, 5, 7, 16]    0.022222\n",
      "[9, 1, 8, 4, 6, 3, 10, 7]                       0.022222\n",
      "Name: proportion, dtype: float64\n",
      "Valid set:\n",
      "diagnosis\n",
      "[18, 8, 4, 3, 6, 7, 5, 16]              0.111111\n",
      "[9, 18, 8, 3, 6, 7, 10, 5, 16]          0.111111\n",
      "[18, 2, 8, 4, 3, 7, 10, 5, 16]          0.111111\n",
      "[1, 3, 7, 10, 17]                       0.111111\n",
      "[9, 12, 7, 8, 4, 3, 6, 1, 10, 5, 17]    0.111111\n",
      "[9, 18, 2, 12, 8, 4, 3, 7]              0.111111\n",
      "[2, 12, 8, 3, 7, 10, 5]                 0.111111\n",
      "[9, 8, 4, 6, 3, 10, 7]                  0.111111\n",
      "[9, 13, 8, 7, 4, 1, 10, 5, 17]          0.111111\n",
      "Name: proportion, dtype: float64\n",
      "mimiciii dataframe pickle file has been loaded\n",
      "value mode :  NV\n",
      "codeemb feature index save_name code_index_NV\n",
      "tokenization for preparing descemb input with NV mode\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "100%|██████████████████████████████████████████| 66/66 [00:00<00:00, 309.06it/s]\n",
      "mimiciii dataframe pickle file has been loaded\n",
      "value mode :  VA\n",
      "codeemb feature index save_name code_index_VA\n",
      "tokenization for preparing descemb input with VA mode\n",
      "100%|██████████████████████████████████████████| 66/66 [00:00<00:00, 293.36it/s]\n",
      "mimiciii dataframe pickle file has been loaded\n",
      "value mode :  DSVA\n",
      "codeemb feature index save_name code_index_DSVA\n",
      "tokenization for preparing descemb input with DSVA mode\n",
      "100%|██████████████████████████████████████████| 66/66 [00:00<00:00, 301.67it/s]\n",
      "[Processing] transform token_type_ids with value encoding\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done  29 tasks      | elapsed:    6.8s\n",
      "[Parallel(n_jobs=-1)]: Done  42 out of  66 | elapsed:    7.3s remaining:    4.1s\n",
      "[Parallel(n_jobs=-1)]: Done  49 out of  66 | elapsed:    7.4s remaining:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done  56 out of  66 | elapsed:    7.7s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done  63 out of  66 | elapsed:    7.8s remaining:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done  66 out of  66 | elapsed:    7.8s finished\n",
      "[End] transform np.array into DataFrame\n",
      "mimiciii dataframe pickle file has been loaded\n",
      "value mode :  VC\n",
      "codeemb feature index save_name code_index_VC\n",
      "tokenization for preparing descemb input with VC mode\n",
      "100%|██████████████████████████████████████████| 66/66 [00:00<00:00, 215.13it/s]\n",
      "preprocessing for mimiciii has been done.\n"
     ]
    }
   ],
   "source": [
    "data = 'mimiciii'\n",
    "data_src_directory = 'datasets/data_input_path/mimic'\n",
    "# data = 'eicu' # uncomment this line if you want to preprocess eicu data\n",
    "# data_src_directory = 'datasets/data_input_path/eicu' # uncomment this line if you want to preprocess eicu data\n",
    "run_ready_directory = 'datasets/data_output_path/mlm'\n",
    "ccs_dx_tool_path = 'datasets/data_input_path/ccs_multi_dx_tool_2015.csv'\n",
    "icd10to9_path = 'datasets/data_input_path/icd10cmtoicd9gem.csv'\n",
    "\n",
    "# preprocess the data\n",
    "!python3 preprocess/preprocess_main.py --src_data {data} --dataset_path {data_src_directory} --dest_path {run_ready_directory} --ccs_dx_tool_path {ccs_dx_tool_path} --icd10to9_path {icd10to9_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341ec0b-4fc9-4e72-9e99-fa355237c294",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- **Computational Resources**: Preprocessing is computationally intensive. The machine configuration (e.g., CPU cores and RAM) should match the recommended specifications (described in the **Training** section below).\n",
    "- **Data Security and Compliance**: Always comply with the licensing agreements of the data sources, particularly regarding the handling and privacy of sensitive healthcare data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885ea37-c50d-4a19-84f5-dac5d5b1cbce",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### References and Links\n",
    "\n",
    "**Citation to the original paper**: [Unifying Heterogeneous Electronic Health Records Systems via Text-Based Code Embedding](https://arxiv.org/abs/2108.03625)\n",
    "\n",
    "**Link to the paper's Github repo**: [Visit the repository](https://github.com/hoon9405/DescEmb?tab=readme-ov-file)\n",
    "\n",
    "\n",
    "### Model Description\n",
    "The DescEmb (Description-based Embedding) model utilizes advanced NLP techniques to handle heterogeneous data from Electronic Health Records (EHRs) systems. The two key components used in training these models are:\n",
    "\n",
    "- **Masked Language Modeling (MLM)**: This approach is inspired by BERT (Bidirectional Encoder Representations from Transformers) and is used to pre-train the DescEmb model. It helps the model learn contextual relationships between words in medical notes by predicting randomly masked words in a sentence.\n",
    "- **Word2Vec Embedding**: This method is used for training a CodeEmb model, which focuses on learning vector representations of medical codes. Unlike MLM, Word2Vec directly predicts surrounding words given a target word, which helps capture the semantic relationships between different medical codes.\n",
    "\n",
    "These methods are crucial for enabling the DescEmb model to generate embeddings that can unify disparate EHR systems, allowing for improved performance on various predictive tasks such as readmission, mortality, and length of stay predictions.\n",
    "\n",
    "### Implementation Code\n",
    "Below are code snippets for pre-training and fine-tuning the models.\n",
    "\n",
    "#### Pre-training the DescEmb Model with MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "31aad11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-06 23:11:05 | INFO numexpr.utils Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.)))\n",
      "2024-05-06 23:11:05 | INFO numexpr.utils NumExpr defaulting to 8 threads.)))\n",
      "[2024-05-06 23:11:06,013][trainers.trainer][INFO] - {'batch_size': 128,\n",
      " 'bert_model': 'bert_tiny',\n",
      " 'device_ids': [0],\n",
      " 'disable_validation': False,\n",
      " 'distributed_world_size': 1,\n",
      " 'dropout': 0.3,\n",
      " 'embed_model': None,\n",
      " 'enc_embed_dim': 128,\n",
      " 'enc_hidden_dim': 256,\n",
      " 'eval_data': None,\n",
      " 'fold': None,\n",
      " 'init_bert_params': False,\n",
      " 'init_bert_params_with_freeze': False,\n",
      " 'input_path': '/Users/genehorecka/Documents/01 '\n",
      "               'UIUC/CS598/Project/cs598_descemb_project/datasets/data_output_path',\n",
      " 'load_pretrained_weights': False,\n",
      " 'lr': 0.0001,\n",
      " 'max_event_len': 150,\n",
      " 'mlm_prob': 0.3,\n",
      " 'model': 'descemb_bert',\n",
      " 'model_path': None,\n",
      " 'n_epochs': 1000,\n",
      " 'patience': 5,\n",
      " 'pred_embed_dim': 128,\n",
      " 'pred_hidden_dim': 256,\n",
      " 'pred_model': None,\n",
      " 'ratio': '100',\n",
      " 'rnn_layer': 1,\n",
      " 'save_dir': 'checkpoints',\n",
      " 'save_prefix': 'checkpoint',\n",
      " 'seed': 1,\n",
      " 'src_data': 'mimiciii',\n",
      " 'task': 'mlm',\n",
      " 'transfer': False,\n",
      " 'valid_subsets': [],\n",
      " 'value_mode': 'NV'}\n",
      "/Users/genehorecka/miniconda3/envs/pytorchenv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[2024-05-06 23:11:06,497][trainers.trainer][INFO] - BertTextEncoder(\n",
      "  (model): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 128)\n",
      "      (token_type_embeddings): Embedding(2, 128)\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-1): 2 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (mlm_proj): Linear(in_features=128, out_features=28996, bias=True)\n",
      "  (post_encode_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      ")\n",
      "[2024-05-06 23:11:06,498][trainers.trainer][INFO] - task: mlm\n",
      "[2024-05-06 23:11:06,498][trainers.trainer][INFO] - model: BertTextEncoder\n",
      "[2024-05-06 23:11:06,499][trainers.trainer][INFO] - num. model params: 8,142,916 (num. trained: 8,142,916)\n",
      "[2024-05-06 23:11:06,963][datasets.dataset][INFO] - loaded 150 train samples\n",
      "[2024-05-06 23:11:07,217][trainers.trainer][INFO] - begin training epoch 1\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "!python main.py \\\n",
    "    --distributed_world_size 1 \\\n",
    "    --input_path '/content/cs598_descemb_project/datasets/data_output_path' \\\n",
    "    --src_data 'mimiciii' \\\n",
    "    --task mlm \\\n",
    "    --mlm_prob 0.3 \\\n",
    "    --model 'descemb_bert'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebceb7-6c0b-40ab-b1f8-a1970189d38b",
   "metadata": {},
   "source": [
    "#### Pre-train a CodeEmb model with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ca27b-7aae-41fa-b1cc-827b6d621cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STDOUT: 2024-05-06 22:18:16 | INFO numexpr.utils Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.)))\n",
      "2024-05-06 22:18:16 | INFO numexpr.utils NumExpr defaulting to 8 threads.)))\n",
      "\n",
      "STDERR: Traceback (most recent call last):\n",
      "  File \"/Users/genehorecka/Documents/01 UIUC/CS598/Project/cs598_descemb_project/main.py\", line 194, in <module>\n",
      "    main()\n",
      "  File \"/Users/genehorecka/Documents/01 UIUC/CS598/Project/cs598_descemb_project/main.py\", line 135, in main\n",
      "    trainer = Word2VecTrainer(args)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/genehorecka/Documents/01 UIUC/CS598/Project/cs598_descemb_project/trainers/word2vec_trainer.py\", line 40, in __init__\n",
      "    vocab_dict = self.vocab_load(args.input_path, args.src_data, args.value_mode)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/genehorecka/Documents/01 UIUC/CS598/Project/cs598_descemb_project/trainers/word2vec_trainer.py\", line 116, in vocab_load\n",
      "    with open(vocab_path, 'rb') as file:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'datasets/data_output_path/mimiciii/code_index_NV_vocab.pkl'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pretrain_codeemb(world_size, input_path, src_data, task, mlm_prob=0.3):\n",
    "    \"\"\"\n",
    "    Run a Python training script using subprocess module.\n",
    "    \n",
    "    Args:\n",
    "    world_size (int): Number of processes to distribute the workload across.\n",
    "    input_path (str): Path to the training data.\n",
    "    src_data (str): Source data identifier, e.g., 'mimic' or 'eicu'.\n",
    "    task (str): Task to perform, e.g., 'w2v' for Word2Vec.\n",
    "    \"\"\"\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', str(world_size),\n",
    "        '--input_path', input_path,\n",
    "        '--src_data', src_data,\n",
    "        '--task', task,\n",
    "        '--model', 'codeemb'\n",
    "    ]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Example usage\n",
    "pretrain_codeemb(world_size=1, input_path='datasets/data_output_path/', src_data='mimiciii', task='w2v')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564e8fa-f54d-4caf-be33-2c626eefbb5e",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "- `data` should be set to `mimic` or `eicu`\n",
    "- `percent` should be set to probability (default: `0.3`) of masking for MLM\n",
    "- `model` should be set to `descemb_bert` or `descemb_rnn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52dcbd7-9c64-4b7e-9eb0-cda8d9e97323",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Computational Requirements\n",
    "The computational requirements for training the DescEmb model, based on the information from the README.md and the paper, include:\n",
    "\n",
    "- **Processor**: Training is computationally intensive and recommended to be performed on a system with at least 128 cores of an AMD EPYC 7502 32-Core Processor for efficient processing.\n",
    "- **Memory**: At least 60GB of RAM is required due to the large size of the datasets and the complexity of the model architectures involved.\n",
    "- **Software**: Python 3.7 or higher with PyTorch 1.8.1 or higher is required. Make sure all dependencies from the `environment.yml` are installed.\n",
    "\n",
    "These requirements ensure that the model training can proceed without hardware-induced limitations, particularly for this task, which involves large datasets like MIMIC-III and eICU.\n",
    "\n",
    "### Implementation Code\n",
    "\n",
    "Below are Python code snippets that demonstrate how to train and fine-tune the models using PyTorch. These examples are based on the commands found in the README.md:\n",
    "\n",
    "#### Training a New Model\n",
    "\n",
    "Other configurations will set to be default, which were used in the DescEmb paper.\n",
    "\n",
    "`$descemb` should be 'descemb_bert' or 'descemb_rnn'\n",
    "\n",
    "`$ratio` should be set to one of [10, 30, 50, 70, 100] (default: 100)\n",
    "\n",
    "`$value` should be set to one of ['NV', 'VA', 'DSVA', 'DSVA_DPE', 'VC']\n",
    "\n",
    "`$task` should be set to one of ['readmission', 'mortality', 'los_3day', 'los_7day', 'diagnosis']\n",
    "\n",
    "Note that `--input-path ` should be the root directory containing preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6797e4-8738-4f94-9828-f7848690fde4",
   "metadata": {},
   "source": [
    "##### Training a New CodeEmb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd426595-77d0-498d-bf93-177003d1782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_new_codeemb_model(input_path, data_source, task, ratio, value_mode):\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', '1',  # Adjust based on the setup, if more GPUs/CPU cores are available\n",
    "        '--input_path', input_path,\n",
    "        '--model', 'ehr_model',\n",
    "        '--embed_model', 'codeemb',\n",
    "        '--pred_model', 'rnn',  # Define the prediction model architecture, change as needed\n",
    "        '--src_data', data_source,  # Specify data source, e.g., 'mimic' or 'eicu'\n",
    "        '--ratio', str(ratio),  # Ratio for data split or sampling, e.g., 10, 30, 50, 70, 100\n",
    "        '--value_mode', value_mode,  # Mode of data processing or feature engineering, e.g., 'NV', 'VA', 'DSVA', 'DSVA_DPE', 'VC'\n",
    "        '--task', task  # Task to perform, e.g., 'readmission', 'mortality', 'los_3day', 'los_7day', 'diagnosis'\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "\n",
    "# Example training new DescEmb model:\n",
    "# train_new_codeemb_model('data_output_path/mimic', 'mimic', 'readmission', 100, 'DSVA')\n",
    "# train_new_codeemb_model('data_output_path/eicu', 'eicu', 'mortality', 70, 'VA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd4f4a-955c-42e4-86ae-3442528942fd",
   "metadata": {},
   "source": [
    "##### Training a New DescEmb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ed9f9-62bc-4abf-8b19-fc524b12cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_new_descemb_model(input_path, data_source, task, ratio, value_mode):\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', '1',  # Adjust based on the setup, if more GPUs/CPU cores are available\n",
    "        '--input_path', input_path,\n",
    "        '--model', 'ehr_model',\n",
    "        '--embed_model', 'descemb',  # Possible values: 'descemb', 'descemb_bert', 'descemb_rnn'\n",
    "        '--pred_model', 'rnn',  # Define the prediction model architecture, change as needed\n",
    "        '--src_data', data_source,  # Specify data source, e.g., 'mimic' or 'eicu'\n",
    "        '--ratio', str(ratio),  # Ratio for data split or sampling, e.g., 10, 30, 50, 70, 100\n",
    "        '--value_mode', value_mode,  # Mode of data processing or feature engineering, e.g., 'NV', 'VA', 'DSVA', 'DSVA_DPE', 'VC'\n",
    "        '--task', task  # Task to perform, e.g., 'readmission', 'mortality', 'los_3day', 'los_7day', 'diagnosis'\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "\n",
    "# Example training new DescEmb model:\n",
    "# train_new_descemb_model('data_output_path/mimic', 'mimic', 'readmission', 100, 'DSVA')\n",
    "# train_new_descemb_model('data_output_path/eicu', 'eicu', 'mortality', 70, 'VA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e6074-dba5-4ea6-ab17-2242a4233e13",
   "metadata": {},
   "source": [
    "**Note**: if you want to train with pre-trained BERT model, add command line parameters `--init_bert_params` or `--init_bert_params_with_freeze`. `--init_bert_params_with_freeze` enables the model to load and freeze BERT parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16384f0e-3812-4117-aea5-df6f7447e016",
   "metadata": {},
   "source": [
    "#### Fine-tune a Pre-Trained Model\n",
    "\n",
    "##### Fine-tuning a Pre-trained CodeEmb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff972cd9-10d7-4544-9c5e-75da8583172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_pretrained_codeemb_model(input_path, model_path, data_source, task, ratio, value_mode, world_size=1, model='ehr_model', embed_model='codeemb', pred_model='rnn'):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained CodeEmb model using subprocess.\n",
    "\n",
    "    Args:\n",
    "    input_path (str): Base path to the training data.\n",
    "    model_path (str): Path to the pre-trained model.\n",
    "    data_source (str): Data source identifier, e.g., 'mimic' or 'eicu'.\n",
    "    task (str): Task for the model to perform.\n",
    "    ratio (int): Ratio of data to use for fine-tuning.\n",
    "    value_mode (str): Value mode to use, e.g., 'NV', 'VA', 'DSVA', 'DSVA_DPE', 'VC'.\n",
    "    world_size (int): Number of processes to distribute the workload across (default: 1).\n",
    "    model (str): High-level model architecture (default: 'ehr_model').\n",
    "    embed_model (str): Type of embedding model, typically 'codeemb' (default: 'codeemb').\n",
    "    pred_model (str): Prediction model, typically 'rnn' (default: 'rnn').\n",
    "    \"\"\"\n",
    "    full_input_path = f'{input_path}/{data_source}'\n",
    "\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', str(world_size),\n",
    "        '--input_path', full_input_path,\n",
    "        '--model_path', model_path,\n",
    "        '--load_pretrained_weights',\n",
    "        '--model', model,\n",
    "        '--embed_model', embed_model,\n",
    "        '--pred_model', pred_model,\n",
    "        '--src_data', data_source,\n",
    "        '--ratio', str(ratio),\n",
    "        '--value_mode', value_mode,\n",
    "        '--task', task\n",
    "    ]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Example usage:\n",
    "# fine_tune_pretrained_codeemb_model('/path/to/data', '/path/to/model.pt', 'mimic', 'mortality', 100, 'DSVA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66773dd-b2f5-4f1d-98f4-befdb14fa9be",
   "metadata": {},
   "source": [
    "##### Fine-tuning a Pre-trained DescEmb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0fb247-8100-47e3-ade9-b7b5749e4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_pretrained_descemb_model(input_path, model_path, data_source, task, ratio, value_mode, world_size=1, embed_model='descemb'):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained DescEmb model using subprocess.\n",
    "\n",
    "    Args:\n",
    "    input_path (str): Base path to the training data.\n",
    "    model_path (str): Path to the pre-trained model.\n",
    "    data_source (str): Data source identifier, e.g., 'mimic' or 'eicu'.\n",
    "    task (str): Task for the model to perform.\n",
    "    ratio (int): Ratio of data to use for fine-tuning.\n",
    "    value_mode (str): Value mode to use, e.g., 'NV', 'VA', 'DSVA', 'DSVA_DPE', 'VC'.\n",
    "    world_size (int): Number of processes to distribute the workload across (default: 1).\n",
    "    embed_model (str): Embedding model to use, typically 'descemb' (default: 'descemb').\n",
    "    \"\"\"\n",
    "    full_input_path = f'{input_path}/{data_source}'\n",
    "\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', str(world_size),\n",
    "        '--input_path', full_input_path,\n",
    "        '--model_path', model_path,\n",
    "        '--load_pretrained_weights',\n",
    "        '--model', 'ehr_model',\n",
    "        '--embed_model', embed_model,\n",
    "        '--pred_model', 'rnn',\n",
    "        '--src_data', data_source,\n",
    "        '--ratio', str(ratio),\n",
    "        '--value_mode', value_mode,\n",
    "        '--task', task\n",
    "    ]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Example usage:\n",
    "# fine_tune_pretrained_descemb_model('data_output_path', '/path/to/model.pt', 'mimic', 'mortality', 100, 'DSVA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e5d5d-b89b-4aff-a0e5-52d6e1cff492",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The primary metrics used in the `README.md` and the associated paper are **Area Under the Precision-Recall Curve (AUPRC)**.\n",
    "\n",
    "### Metrics Descriptions\n",
    "\n",
    "#### Area Under the Precision-Recall Curve (AUPRC):\n",
    "\n",
    "- **Precision (Positive Predictive Value)**: The ratio of true positive predictions to the total predicted positives. It shows the accuracy of the positive predictions.\n",
    "- **Recall (Sensitivity)**: The ratio of true positives to the actual total positives in the dataset. It measures the model's ability to capture positive instances.\n",
    "- **AUPRC**: The AUPRC is a single number summary of these two metrics across different thresholds, emphasizing the balance between precision and recall. It is especially valuable in medical predictions where the cost of false negatives is high.\n",
    "\n",
    "### Implementation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a6394-9423-4704-924e-e5f5df531e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_auprc(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Calculate the Area Under the Precision-Recall Curve (AUPRC).\n",
    "    \n",
    "    Args:\n",
    "    y_true (list or array): True binary labels in range {0, 1}.\n",
    "    y_scores (list or array): Target scores, can either be probability estimates of the positive class,\n",
    "                              confidence values, or non-thresholded measure of decisions.\n",
    "    \n",
    "    Returns:\n",
    "    float: AUPRC score\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    auprc = auc(recall, precision)\n",
    "    return auprc\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Plot the Precision-Recall curve for a given set of true labels and scores.\n",
    "    \n",
    "    Args:\n",
    "    y_true (list or array): True binary labels in range {0, 1}.\n",
    "    y_scores (list or array): Target scores, similar to calculate_auprc.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label=f'AUPRC = {auc(recall, precision):.2f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "# Example evaluation usage\n",
    "# auprc_score = calculate_auprc(y_true, y_scores)\n",
    "# plot_precision_recall_curve(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2162a-9396-4a61-a4ac-667f70d0c824",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- Ensure that the `y_true` and `y_scores` are correctly formatted as arrays of true labels and model predictions respectively.\n",
    "- The plot_precision_recall_curve function provides a visual understanding of the trade-off between precision and recall for different threshold settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde43ed-cdbb-4c34-a9bc-4ac570073001",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Results Overview\n",
    "\n",
    "The DescEmb models demonstrated superior or comparable performance to traditional code-based embeddings (CodeEmb) across various clinical prediction tasks. The models were evaluated on tasks such as predicting readmission, mortality, length of stay (both 3-day and 7-day), and diagnosis predictions using datasets like MIMIC-III and eICU.\n",
    "\n",
    "## Analyses\n",
    "\n",
    "- **Performance Gains**: DescEmb models, especially those leveraging BERT-based embeddings, showed consistent improvements in AUPRC (Area Under the Precision-Recall Curve) across most tasks compared to traditional models.\n",
    "- **Model Comparisons**: BERT-based DescEmb models generally outperformed simpler RNN-based models in complex tasks like diagnosis prediction, highlighting the effectiveness of pre-trained language models in handling complex textual data from EHRs.\n",
    "- **Impact of Pre-training**: The addition of Masked Language Modeling (MLM) pre-training marginally improved performance, suggesting that further domain-specific adaptation of language models could be beneficial.\n",
    "\n",
    "## Plans\n",
    "\n",
    "Moving forward, the research can focus on:\n",
    "- **Further Optimization**: Enhancing the efficiency of the models to make them accessible for real-time applications in clinical settings.\n",
    "- **Expanding Dataset Usage**: Applying the DescEmb framework to additional datasets and exploring its effectiveness across different healthcare systems.\n",
    "- **Advanced Model Architectures**: Investigating the integration of more complex neural architectures and their impact on the performance of EHR-based predictive models.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The DescEmb approach marks a significant step forward in the use of NLP techniques for EHR data, offering a promising avenue for enhancing predictive healthcare analytics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
