{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12eeb976-aec8-4203-979f-ed85033f9aa9",
   "metadata": {},
   "source": [
    "# CS598 Project: Enhancing Healthcare Predictive Models through Text-Based EHR Code Embedding\n",
    "\n",
    "**Name:** Gene Horecka  \n",
    "**Email:** [geneeh2@illinois.edu](mailto:geneeh2@illinois.edu)  \n",
    "**Course:** CS 598 Deep Learning for Healthcare - Spring 2024\n",
    "## Project Github Link: [https://github.com/genefever/cs598_descemb_project](https://github.com/genefever/cs598_descemb_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c719c0ca",
   "metadata": {},
   "source": [
    "\n",
    "# Introduction\n",
    "\n",
    "The paper \"[Unifying Heterogeneous Electronic Health Records Systems via Text-Based Code Embedding](https://arxiv.org/abs/2108.03625)\" addresses the significant challenge of heterogeneity in Electronic Health Records (EHR) systems. These systems, essential for modern healthcare, often differ in their coding and formatting of medical data, which hampers the development and application of predictive models across different institutions or datasets.\n",
    "\n",
    "The primary contribution of this paper is the development of a novel framework named [Description-based Embedding (DescEmb)](https://github.com/hoon9405/DescEmb). This framework uses neural language models to create a unified, code-agnostic text-based representation of medical data. By transforming various coding formats into a consistent text-based embedding, DescEmb allows for more flexible and effective application of deep learning models across diverse EHR systems. This approach notably enhances the performance of predictive healthcare models, demonstrating superior results in several experimental setups compared to traditional code-based embedding methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64856b9",
   "metadata": {},
   "source": [
    "\n",
    "# Scope of Reproducibility\n",
    "\n",
    "The scope of reproducibility for the paper \"[Unifying Heterogeneous Electronic Health Records Systems via Text-Based Code Embedding](https://arxiv.org/abs/2108.03625)\" entails verifying the claims of improved predictive performance through the implementation of the DescEmb framework. This framework leverages a neural language model to convert medical codes into a unified, text-based embedding, which is purported to enhance predictive healthcare research without the constraints imposed by diverse EHR systems.\n",
    "\n",
    "## Key Claims for Reproduction:\n",
    "1. **Unified Learning Across Diverse EHR Formats:** DescEmb can unify learning across various EHR systems without needing individualized pre-processing or domain-specific knowledge, due to its text-based nature.\n",
    "2. **Superior Predictive Performance:** The framework demonstrates better or comparable predictive performance than traditional code-based approaches across several clinical prediction tasks.\n",
    "3. **Efficient Deployment in Diverse Environments:** The text-based approach allows models trained with DescEmb to be easily transferred and applied across different hospitals with differing EHR systems.\n",
    "\n",
    "The reproducibility effort will focus on these claims by attempting to replicate the experiments outlined in the original paper using the datasets and code provided in the project's [GitHub repository](https://github.com/hoon9405/DescEmb). The process will involve re-running the model training and evaluation procedures to verify the reported performance improvements and the operational flexibility of the DescEmb approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ee017-b71e-4406-ba6a-ff4abe8fc620",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "## Setting Up the Environment\n",
    "\n",
    "To replicate the preprocessing and modeling described in the project, the following environment must be set up:\n",
    "\n",
    "1. **Conda Environment**: Use the provided `environment.yml` file to create a Conda environment. This will install all required dependencies, including Python and PyTorch. Run the following command in your terminal:\n",
    "\n",
    "   ```bash\n",
    "   conda env create -f environment.yml\n",
    "\n",
    "\n",
    "\n",
    "2. **Activate the Environment**\n",
    "\n",
    "   ```bash\n",
    "   conda activate descemb\n",
    "\n",
    "## Data\n",
    "\n",
    "### Data Description\n",
    "\n",
    "The datasets used in this project are MIMIC-III and eICU, which are publicly available on the PhysioNet repository. These datasets include comprehensive data from intensive care units (ICUs), such as time-stamped records of medical events, lab results, medications, and more, recorded in different medical code systems.\n",
    "\n",
    "- [**MIMIC-III**](https://physionet.org/content/iii/1.4/): Contains data for over 60,000 ICU stays at Beth Israel Deaconess Medical Center between 2001 and 2012. It includes information such as lab measurements, medication orders, and diagnostic codes.\n",
    "- [**eICU**](https://physionet.org/content/eicu-crd/2.0/): A multi-center dataset containing data for over 200,000 ICU stays across the United States between 2014 and 2015. It includes similar types of data to MIMIC-III but is structured differently.\n",
    "- [**ccs_multi_dx_tool_2015**](https://www.hcup-us.ahrq.gov/toolssoftware/ccs/Multi_Level_CCS_2015.zip): The Clinical Classifications Software (CCS) 2015 dataset groups ICD-9-CM diagnosis and procedure codes into clinically meaningful categories that are useful for health data analysis and research.\n",
    "- [**icd10cmtoicd9gem**](https://data.nber.org/gem/icd10cmtoicd9gem.csv): The `icd10cmtoicd9gem.csv` file is a mapping table that converts ICD-10-CM codes to ICD-9-CM codes.\n",
    "\n",
    "### Data Access\n",
    "\n",
    "The datasets utilized in this project, MIMIC-III and eICU, are publicly available via PhysioNet. Users must adhere to licensing agreements and data usage policies, including the requirement for completing a training course on data handling. Detailed instructions for data access are as follows:\n",
    "\n",
    "- **MIMIC-III** and **eICU**: Access these datasets by registering and completing the required data usage agreement at [PhysioNet](https://physionet.org/). After gaining access, download the data directly from their respective project pages.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "The preparation of the datasets for training involves several steps, from downloading the data to preprocessing it into a usable format. Here’s how you can prepare your data:\n",
    "\n",
    "1. **Download the Data**: After obtaining the necessary permissions, download the datasets from PhysioNet.\n",
    "2. **Organize the Data**: Arrange the downloaded files according to the directory structure below:\n",
    "```\n",
    "data_input_path\n",
    "├─ mimic\n",
    "│  ├─ ADMISSIONS.csv\n",
    "│  ├─ PATIENTS.csv\n",
    "│  ├─ ICUSYAYS.csv\n",
    "│  ├─ LABEVENTES.csv\n",
    "│  ├─ PRESCRIPTIONS.csv\n",
    "│  ├─ PROCEDURES.csv\n",
    "│  ├─ INPUTEVENTS_CV.csv\n",
    "│  ├─ INPUTEVENTS_MV.csv\n",
    "│  ├─ D_ITEMDS.csv\n",
    "│  ├─ D_ICD_PROCEDURES.csv\n",
    "│  └─ D_LABITEMBS.csv\n",
    "├─ eicu\n",
    "│  ├─ diagnosis.csv\n",
    "│  ├─ infusionDrug.csv\n",
    "│  ├─ lab.csv\n",
    "│  ├─ medication.csv\n",
    "│  └─ patient.csv\n",
    "├─ ccs_multi_dx_tool_2015.csv\n",
    "└─ icd10cmtoicd9gem.csv\n",
    "\n",
    "```\n",
    "```\n",
    "data_output_path\n",
    "├─mimic\n",
    "├─eicu\n",
    "├─pooled\n",
    "├─label\n",
    "└─fold\n",
    "```\n",
    "\n",
    "3. **Preprocess the Data**: Use the `preprocess_run.sh` script to execute the preprocessing steps. This script automates the process of converting raw datasets into a format ready for model training. Adjust paths in the script according to your directory setup:\n",
    "\n",
    "   ```bash\n",
    "   bash preprocess_run.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0b403d-6344-45ca-ad3e-cceec415c6fd",
   "metadata": {},
   "source": [
    "### Implementation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5a1a031b-db1c-4aed-afd7-ef00d7201758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Define the source and destination paths\n",
    "src_data_paths = [\n",
    "    'data_input_path/mimic',\n",
    "    'data_input_path/eicu',\n",
    "    'data_input_path/ccs_multi_dx_tool_2015.csv',\n",
    "    'data_input_path/icd10cmtoicd9gem.csv'\n",
    "]\n",
    "dest_path = 'data_output_path'\n",
    "\n",
    "# Commabnd to run preprocessing for each dataset\n",
    "\n",
    "# for src_data in src_data_paths:\n",
    "#     command = [\n",
    "#         'python', '../preprocess/preprocess_main.py',\n",
    "#         '--src_data', src_data.split('/')[-1],  # Gets the last part of the path for clarity\n",
    "#         '--dataset_path', src_data,\n",
    "#         '--dest_path', dest_path\n",
    "#     ]\n",
    "#     subprocess.run(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2341ec0b-4fc9-4e72-9e99-fa355237c294",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- **Computational Resources**: Preprocessing is computationally intensive. The machine configuration (e.g., CPU cores and RAM) should match the recommended specifications (described in the **Training** section belowO).\n",
    "- **Data Security and Compliance**: Always comply with the licensing agreements of the data sources, particularly regarding the handling and privacy of sensitive healthcare data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5885ea37-c50d-4a19-84f5-dac5d5b1cbce",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "### Model Description\n",
    "The DescEmb (Description-based Embedding) model utilizes advanced NLP techniques to handle heterogeneous data from Electronic Health Records (EHRs) systems. The two key components used in training these models are:\n",
    "\n",
    "- **Masked Language Modeling (MLM)**: This approach is inspired by BERT (Bidirectional Encoder Representations from Transformers) and is used to pre-train the DescEmb model. It helps the model learn contextual relationships between words in medical notes by predicting randomly masked words in a sentence.\n",
    "- **Word2Vec Embedding**: This method is used for training a CodeEmb model, which focuses on learning vector representations of medical codes. Unlike MLM, Word2Vec directly predicts surrounding words given a target word, which helps capture the semantic relationships between different medical codes.\n",
    "\n",
    "These methods are crucial for enabling the DescEmb model to generate embeddings that can unify disparate EHR systems, allowing for improved performance on various predictive tasks such as readmission, mortality, and length of stay predictions.\n",
    "\n",
    "### Implementation Code\n",
    "Below are code snippets for pre-training and fine-tuning the models.\n",
    "\n",
    "#### Pre-training the DescEmb Model with MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c61e0901-1a71-407a-8f18-172186c46a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-training a DescEmb model using Masked Language Modeling (MLM)\n",
    "\n",
    "def pretrain_descemb_model(world_size, input_path, src_data, mlm_prob, model):\n",
    "    \"\"\"\n",
    "    Pre-train a DescEmb model using Masked Language Modeling (MLM) with the subprocess module.\n",
    "    \n",
    "    Args:\n",
    "    world_size (int): Number of processes to distribute the workload across.\n",
    "    input_path (str): Path to the training data.\n",
    "    src_data (str): Source data identifier, e.g., 'mimic' or 'eicu'.\n",
    "    mlm_prob (float): Probability of masking for MLM.\n",
    "    model (str): Model to use, e.g., 'descemb_bert' or 'descemb_rnn'.\n",
    "    \"\"\"\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', str(world_size),\n",
    "        '--input_path', input_path,\n",
    "        '--src_data', src_data,\n",
    "        '--task', 'mlm',\n",
    "        '--mlm_prob', str(mlm_prob),\n",
    "        '--model', model\n",
    "    ]\n",
    "    # Execute the command\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    # Print standard output and error\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Example usage:\n",
    "# You need to replace '/path/to/data', 'mimic', 0.3, 'descemb_bert' with actual values as needed.\n",
    "# pretrain_descemb_model(world_size=1, input_path='/path/to/data', src_data='mimic', mlm_prob=0.3, model='descemb_bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ebceb7-6c0b-40ab-b1f8-a1970189d38b",
   "metadata": {},
   "source": [
    "#### Pre-train a CodeEmb model with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "127ca27b-7aae-41fa-b1cc-827b6d621cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_codeemb(world_size, input_path, src_data, task, mlm_prob=0.3):\n",
    "    \"\"\"\n",
    "    Run a Python training script using subprocess module.\n",
    "    \n",
    "    Args:\n",
    "    world_size (int): Number of processes to distribute the workload across.\n",
    "    input_path (str): Path to the training data.\n",
    "    src_data (str): Source data identifier, e.g., 'mimic' or 'eicu'.\n",
    "    task (str): Task to perform, e.g., 'w2v' for Word2Vec.\n",
    "    \"\"\"\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', str(world_size),\n",
    "        '--input_path', input_path,\n",
    "        '--src_data', src_data,\n",
    "        '--task', task,\n",
    "        '--model', 'codeemb'\n",
    "    ]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Example usage\n",
    "# pretrain_codeemb(world_size=1, input_path='data_output_path', src_data='mimic', task='w2v', )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d564e8fa-f54d-4caf-be33-2c626eefbb5e",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "- `data` should be set to `mimic` or `eicu`\n",
    "- `percent` should be set to probability (default: `0.3`) of masking for MLM\n",
    "- `model` should be set to `descemb_bert` or `descemb_rnn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52dcbd7-9c64-4b7e-9eb0-cda8d9e97323",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "### Computational Requirements\n",
    "The computational requirements for training the DescEmb model, based on the information from the README.md and the paper, include:\n",
    "\n",
    "- **Processor**: Training is computationally intensive and recommended to be performed on a system with at least 128 cores of an AMD EPYC 7502 32-Core Processor for efficient processing.\n",
    "- **Memory**: At least 60GB of RAM is required due to the large size of the datasets and the complexity of the model architectures involved.\n",
    "- **Software**: Python 3.7 or higher with PyTorch 1.8.1 or higher is required. Make sure all dependencies from the `environment.yml` are installed.\n",
    "\n",
    "These requirements ensure that the model training can proceed without hardware-induced limitations, particularly for this task, which involves large datasets like MIMIC-III and eICU.\n",
    "\n",
    "### Implementation Code\n",
    "\n",
    "Below are Python code snippets that demonstrate how to train and fine-tune the models using PyTorch. These examples are based on the commands found in the README.md:\n",
    "\n",
    "#### Training a New Model\n",
    "\n",
    "Other configurations will set to be default, which were used in the DescEmb paper.\n",
    "\n",
    "`$descemb` should be 'descemb_bert' or 'descemb_rnn'\n",
    "\n",
    "`$ratio` should be set to one of [10, 30, 50, 70, 100] (default: 100)\n",
    "\n",
    "`$value` should be set to one of ['NV', 'VA', 'DSVA', 'DSVA_DPE', 'VC']\n",
    "\n",
    "`$task` should be set to one of ['readmission', 'mortality', 'los_3day', 'los_7day', 'diagnosis']\n",
    "\n",
    "Note that `--input-path ` should be the root directory containing preprocessed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6797e4-8738-4f94-9828-f7848690fde4",
   "metadata": {},
   "source": [
    "##### Training a New CodeEmb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd426595-77d0-498d-bf93-177003d1782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_new_codeemb_model(input_path, data_source, task, ratio, value_mode):\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', '1',  # Adjust based on the setup, if more GPUs/CPU cores are available\n",
    "        '--input_path', input_path,\n",
    "        '--model', 'ehr_model',\n",
    "        '--embed_model', 'codeemb',\n",
    "        '--pred_model', 'rnn',  # Define the prediction model architecture, change as needed\n",
    "        '--src_data', data_source,  # Specify data source, e.g., 'mimic' or 'eicu'\n",
    "        '--ratio', str(ratio),  # Ratio for data split or sampling, e.g., 10, 30, 50, 70, 100\n",
    "        '--value_mode', value_mode,  # Mode of data processing or feature engineering, e.g., 'NV', 'VA', 'DSVA', 'DSVA_DPE', 'VC'\n",
    "        '--task', task  # Task to perform, e.g., 'readmission', 'mortality', 'los_3day', 'los_7day', 'diagnosis'\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "\n",
    "# Example training new DescEmb model:\n",
    "# train_new_codeemb_model('data_output_path/mimic', 'mimic', 'readmission', 100, 'DSVA')\n",
    "# train_new_codeemb_model('data_output_path/eicu', 'eicu', 'mortality', 70, 'VA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd4f4a-955c-42e4-86ae-3442528942fd",
   "metadata": {},
   "source": [
    "##### Training a New DescEmb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e45ed9f9-62bc-4abf-8b19-fc524b12cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_new_descemb_model(input_path, data_source, task, ratio, value_mode):\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', '1',  # Adjust based on the setup, if more GPUs/CPU cores are available\n",
    "        '--input_path', input_path,\n",
    "        '--model', 'ehr_model',\n",
    "        '--embed_model', 'descemb',  # Possible values: 'descemb', 'descemb_bert', 'descemb_rnn'\n",
    "        '--pred_model', 'rnn',  # Define the prediction model architecture, change as needed\n",
    "        '--src_data', data_source,  # Specify data source, e.g., 'mimic' or 'eicu'\n",
    "        '--ratio', str(ratio),  # Ratio for data split or sampling, e.g., 10, 30, 50, 70, 100\n",
    "        '--value_mode', value_mode,  # Mode of data processing or feature engineering, e.g., 'NV', 'VA', 'DSVA', 'DSVA_DPE', 'VC'\n",
    "        '--task', task  # Task to perform, e.g., 'readmission', 'mortality', 'los_3day', 'los_7day', 'diagnosis'\n",
    "    ]\n",
    "    subprocess.run(command)\n",
    "\n",
    "# Example training new DescEmb model:\n",
    "# train_new_descemb_model('data_output_path/mimic', 'mimic', 'readmission', 100, 'DSVA')\n",
    "# train_new_descemb_model('data_output_path/eicu', 'eicu', 'mortality', 70, 'VA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4e6074-dba5-4ea6-ab17-2242a4233e13",
   "metadata": {},
   "source": [
    "**Note**: if you want to train with pre-trained BERT model, add command line parameters `--init_bert_params` or `--init_bert_params_with_freeze`. `--init_bert_params_with_freeze` enables the model to load and freeze BERT parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16384f0e-3812-4117-aea5-df6f7447e016",
   "metadata": {},
   "source": [
    "#### Fine-tune a Pre-Trained Model\n",
    "\n",
    "##### Fine-tuning a Pre-trained CodeEmb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ff972cd9-10d7-4544-9c5e-75da8583172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_pretrained_codeemb_model(input_path, model_path, data_source, task, ratio, value_mode, world_size=1, model='ehr_model', embed_model='codeemb', pred_model='rnn'):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained CodeEmb model using subprocess.\n",
    "\n",
    "    Args:\n",
    "    input_path (str): Base path to the training data.\n",
    "    model_path (str): Path to the pre-trained model.\n",
    "    data_source (str): Data source identifier, e.g., 'mimic' or 'eicu'.\n",
    "    task (str): Task for the model to perform.\n",
    "    ratio (int): Ratio of data to use for fine-tuning.\n",
    "    value_mode (str): Value mode to use, e.g., 'NV', 'VA', 'DSVA', 'DSVA_DPE', 'VC'.\n",
    "    world_size (int): Number of processes to distribute the workload across (default: 1).\n",
    "    model (str): High-level model architecture (default: 'ehr_model').\n",
    "    embed_model (str): Type of embedding model, typically 'codeemb' (default: 'codeemb').\n",
    "    pred_model (str): Prediction model, typically 'rnn' (default: 'rnn').\n",
    "    \"\"\"\n",
    "    full_input_path = f'{input_path}/{data_source}'\n",
    "\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', str(world_size),\n",
    "        '--input_path', full_input_path,\n",
    "        '--model_path', model_path,\n",
    "        '--load_pretrained_weights',\n",
    "        '--model', model,\n",
    "        '--embed_model', embed_model,\n",
    "        '--pred_model', pred_model,\n",
    "        '--src_data', data_source,\n",
    "        '--ratio', str(ratio),\n",
    "        '--value_mode', value_mode,\n",
    "        '--task', task\n",
    "    ]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Example usage:\n",
    "# fine_tune_pretrained_codeemb_model('/path/to/data', '/path/to/model.pt', 'mimic', 'mortality', 100, 'DSVA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66773dd-b2f5-4f1d-98f4-befdb14fa9be",
   "metadata": {},
   "source": [
    "##### Fine-tuning a Pre-trained DescEmb Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0e0fb247-8100-47e3-ade9-b7b5749e4d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_pretrained_descemb_model(input_path, model_path, data_source, task, ratio, value_mode, world_size=1, embed_model='descemb'):\n",
    "    \"\"\"\n",
    "    Fine-tune a pre-trained DescEmb model using subprocess.\n",
    "\n",
    "    Args:\n",
    "    input_path (str): Base path to the training data.\n",
    "    model_path (str): Path to the pre-trained model.\n",
    "    data_source (str): Data source identifier, e.g., 'mimic' or 'eicu'.\n",
    "    task (str): Task for the model to perform.\n",
    "    ratio (int): Ratio of data to use for fine-tuning.\n",
    "    value_mode (str): Value mode to use, e.g., 'NV', 'VA', 'DSVA', 'DSVA_DPE', 'VC'.\n",
    "    world_size (int): Number of processes to distribute the workload across (default: 1).\n",
    "    embed_model (str): Embedding model to use, typically 'descemb' (default: 'descemb').\n",
    "    \"\"\"\n",
    "    full_input_path = f'{input_path}/{data_source}'\n",
    "\n",
    "    command = [\n",
    "        'python', 'main.py',\n",
    "        '--distributed_world_size', str(world_size),\n",
    "        '--input_path', full_input_path,\n",
    "        '--model_path', model_path,\n",
    "        '--load_pretrained_weights',\n",
    "        '--model', 'ehr_model',\n",
    "        '--embed_model', embed_model,\n",
    "        '--pred_model', 'rnn',\n",
    "        '--src_data', data_source,\n",
    "        '--ratio', str(ratio),\n",
    "        '--value_mode', value_mode,\n",
    "        '--task', task\n",
    "    ]\n",
    "    result = subprocess.run(command, capture_output=True, text=True)\n",
    "    print(\"STDOUT:\", result.stdout)\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "\n",
    "# Example usage:\n",
    "# fine_tune_pretrained_descemb_model('data_output_path', '/path/to/model.pt', 'mimic', 'mortality', 100, 'DSVA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652e5d5d-b89b-4aff-a0e5-52d6e1cff492",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "The primary metrics used in the `README.md` and the associated paper are **Area Under the Precision-Recall Curve (AUPRC)**.\n",
    "\n",
    "### Metrics Descriptions\n",
    "\n",
    "#### Area Under the Precision-Recall Curve (AUPRC):\n",
    "\n",
    "- **Precision (Positive Predictive Value)**: The ratio of true positive predictions to the total predicted positives. It shows the accuracy of the positive predictions.\n",
    "- **Recall (Sensitivity)**: The ratio of true positives to the actual total positives in the dataset. It measures the model's ability to capture positive instances.\n",
    "- **AUPRC**: The AUPRC is a single number summary of these two metrics across different thresholds, emphasizing the balance between precision and recall. It is especially valuable in medical predictions where the cost of false negatives is high.\n",
    "\n",
    "### Implementation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea6a6394-9423-4704-924e-e5f5df531e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_auprc(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Calculate the Area Under the Precision-Recall Curve (AUPRC).\n",
    "    \n",
    "    Args:\n",
    "    y_true (list or array): True binary labels in range {0, 1}.\n",
    "    y_scores (list or array): Target scores, can either be probability estimates of the positive class,\n",
    "                              confidence values, or non-thresholded measure of decisions.\n",
    "    \n",
    "    Returns:\n",
    "    float: AUPRC score\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    auprc = auc(recall, precision)\n",
    "    return auprc\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Plot the Precision-Recall curve for a given set of true labels and scores.\n",
    "    \n",
    "    Args:\n",
    "    y_true (list or array): True binary labels in range {0, 1}.\n",
    "    y_scores (list or array): Target scores, similar to calculate_auprc.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_scores)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label=f'AUPRC = {auc(recall, precision):.2f}')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "# Example evaluation usage\n",
    "# auprc_score = calculate_auprc(y_true, y_scores)\n",
    "# plot_precision_recall_curve(y_true, y_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2162a-9396-4a61-a4ac-667f70d0c824",
   "metadata": {},
   "source": [
    "**Note**\n",
    "- Ensure that the `y_true` and `y_scores` are correctly formatted as arrays of true labels and model predictions respectively.\n",
    "- The plot_precision_recall_curve function provides a visual understanding of the trade-off between precision and recall for different threshold settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde43ed-cdbb-4c34-a9bc-4ac570073001",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Results Overview\n",
    "\n",
    "The DescEmb models demonstrated superior or comparable performance to traditional code-based embeddings (CodeEmb) across various clinical prediction tasks. The models were evaluated on tasks such as predicting readmission, mortality, length of stay (both 3-day and 7-day), and diagnosis predictions using datasets like MIMIC-III and eICU.\n",
    "\n",
    "## Analyses\n",
    "\n",
    "- **Performance Gains**: DescEmb models, especially those leveraging BERT-based embeddings, showed consistent improvements in AUPRC (Area Under the Precision-Recall Curve) across most tasks compared to traditional models.\n",
    "- **Model Comparisons**: BERT-based DescEmb models generally outperformed simpler RNN-based models in complex tasks like diagnosis prediction, highlighting the effectiveness of pre-trained language models in handling complex textual data from EHRs.\n",
    "- **Impact of Pre-training**: The addition of Masked Language Modeling (MLM) pre-training marginally improved performance, suggesting that further domain-specific adaptation of language models could be beneficial.\n",
    "\n",
    "## Plans\n",
    "\n",
    "Moving forward, the research can focus on:\n",
    "- **Further Optimization**: Enhancing the efficiency of the models to make them accessible for real-time applications in clinical settings.\n",
    "- **Expanding Dataset Usage**: Applying the DescEmb framework to additional datasets and exploring its effectiveness across different healthcare systems.\n",
    "- **Advanced Model Architectures**: Investigating the integration of more complex neural architectures and their impact on the performance of EHR-based predictive models.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The DescEmb approach marks a significant step forward in the use of NLP techniques for EHR data, offering a promising avenue for enhancing predictive healthcare analytics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
